#!/usr/bin/env python
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4
import argparse
import re
from login import get_requests_client
from bs4 import BeautifulSoup

urlre = re.compile(
    r'^(?:http|ftp)s?://'
    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+(?:[A-Z]{2,6}\.?|[A-Z0-9-]{2,}\.?)|'
    r'localhost|'
    r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'
    r'(?::\d+)?'
    r'(?:/?|[/?]\S+)$', re.IGNORECASE)

CLIENT = get_requests_client()


def make_url(technology):
    url_base = 'https://egghead.io'
    path = '%s/technologies/%s' % (url_base, technology)
    return path


def get_html_for_page(url, page):
    params = {'order': 'desc', 'page': page}
    r = CLIENT.get(url, params=params)
    return r.text


def find_lesson_links(html):
    soup = BeautifulSoup(html)
    dl_links = [tag['href'] for tag
                in soup.find_all('a', href=re.compile('lessons/.+'))]
    return dl_links


def get_links_for_page(url, page):
    html = get_html_for_page(url, page)
    lesson_links = find_lesson_links(html)
    return lesson_links


def find_all_lessons(technology):
    links = []
    url = make_url(technology)
    page = 1
    lessons_left = True
    while(lessons_left):
        scraped_links = get_links_for_page(url, page)
        url_base = 'https://egghead.io%s'
        if len(scraped_links):
            full_links = [url_base % (link) for link in scraped_links]
            for link in full_links:
                if urlre.search(link):
                    links.append(link)
            page += 1
        else:
            lessons_left = False
    return set(links)


def find_all_s3_links(technology):
    s3_links = []
    lesson_html = []
    lessons = find_all_lessons(technology)
    for lesson in lessons:
        try:
            r = CLIENT.get(lesson)
            lesson_html.append(r.text)
        except:
            print lesson
    for lesson in lesson_html:
        if lesson is not None:
            s3_links.extend(find_s3_links(lesson))
    return set(s3_links)


def find_s3_links(html):
    soup = BeautifulSoup(html)
    aws_re = re.compile("https://egghead-video-downloads.s3.amazonaws.com/lessons/.+")
    dl_links = [tag['href'] for tag
                in soup.find_all('a', href=aws_re)]
    return dl_links


def main(args):
    tech = args.technology
    links = find_all_s3_links(tech)
    return list(links)

if __name__ == '__main__':
    description = 'Lookup instances on a host!!'
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument('technology', help='technology name, like react, \
                         angularjs, d3, or js')
    args = parser.parse_args()
    data = main(args=args)
    print "\n".join(data)
